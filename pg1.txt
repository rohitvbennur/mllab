import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')
df = pd.read_csv('/Users/abhishek_r/Desktop/Programs/ML_LAB/housing.csv')
df.head() #display the first few rows of a DataFrame, providing a quick overview of its structure and contents
df.shape #provides the dimensions of a DataFrame as a tuple, indicating its number of rows and columns.
df.info() #provides a concise summary of a DataFrame, offering insights into its structure and contents.
df.nunique() #returns the number of unique values for each column in a DataFrame.
df.isnull().sum() # identifies missing values within a DataFrame.
df.duplicated().sum() # identifies duplicate rows in a Dataframe.
df['total_bedrooms'].median()
df.head()
df.describe().T
Numerical = df.select_dtypes(include=[np.number]).columns
print(Numerical)
#Plotting Histograms
for col in Numerical:
 plt.figure(figsize=(10, 6))
 df[col].plot(kind='hist', title=col, bins=60, edgecolor='black')
 plt.ylabel('Frequency')
 plt.show()
#Plotting Boxplots
for col in Numerical:
 plt.figure(figsize=(6, 6))
 sns.boxplot(df[col], color='blue')
 plt.title(col)
 plt.ylabel(col)
 plt.show()

program 2
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import fetch_california_housing
#Load California Housing dataset
#data = fetch_california_housing()
df = pd.read_csv('/Users/abhishek_r/Desktop/Programs/ML_LAB/housing.csv')
#Convert to DataFrame
#df = pd.DataFrame(data.data, columns=data.feature_names)
print(df)
df['Target'] = df.median_house_value
# Adding the target variable (median house value)
print("\nBasic Information about Dataset:")
print(df.info()) # Overview of dataset
print("\nFirst Five Rows of Dataset:")
print(df.head()) # Display first few rows
print("\nSummary Statistics:")
print(df.describe()) # Summary statistics of dataset
print("\nMissing Values in Each Column:")
print(df.isnull().sum()) # Count of missing values
plt.figure(figsize=(12, 8))
df.hist(figsize=(12, 8), bins=30, edgecolor='black')
plt.suptitle("Feature Distributions", fontsize=16)
plt.show()
plt.figure(figsize=(12, 6))
sns.boxplot(data=df)
plt.xticks(rotation=45)
plt.title("Boxplots of Features to Identify Outliers")
plt.show()
plt.figure(figsize=(10, 6))
corr_matrix = df.corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title("Feature Correlation Heatmap")
plt.show()
sns.pairplot(df[['median_income', 'housing_median_age', 'total_bedrooms', 'Target']], diag_kind='kde')
plt.show()

program 3
from sklearn.datasets import load_iris
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# Step 1: Load the Iris dataset
iris = load_iris()
features = iris.data # The 4 features: Sepal Length, Sepal Width, Petal Length, Petal Width
target = iris.target
# The target class (species)
# Step 2: Standardize the features
scaler = StandardScaler()
features_standardized = scaler.fit_transform(features)
# Step 3: Apply PCA to reduce to 2 components
pca = PCA(n_components=2)
features_pca = pca.fit_transform(features_standardized)
# Step 4: Create a DataFrame for the reduced data
pca_df = pd.DataFrame(data=features_pca, columns=["Principal Component 1", "Principal Component 2"])
pca_df["Target"] = iris.target
# Step 5: Visualize the PCA results plt.figure(figsize=(8, 6))
for label, color in zip(iris.target_names, ["red", "green", "blue"]):
 plt.scatter(pca_df.loc[pca_df["Target"] == list(iris.target_names).index(label), "Principal Component
1"],pca_df.loc[pca_df["Target"] == list(iris.target_names).index(label), "Principal Component
2"],label=label,alpha=0.7 )
 plt.title("PCA on Iris Dataset (4 features to 2 features)", fontsize=14)
 plt.xlabel("Principal Component 1", fontsize=12)
 plt.ylabel("Principal Component 2", fontsize=12)
 plt.legend(title="Species")
 plt.grid()
 plt.show()
explained_variance = pca.explained_variance_ratio_
print("Explained Variance by each Principal Component:")
print("Principal Component 1: ", explained_variance[0])
print("Principal Component 2: ", explained_variance[1])
print("Total Variance Retained: ", sum(explained_variance))
# Pairplot visualization of the original Iris dataset
iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)
iris_df['species'] = iris.target
sns.pairplot(iris_df, hue='species', palette='bright')
plt.suptitle("Pairplot of Original Iris Dataset", y=1.02)
plt.show()

program 4
import pandas as pd
import numpy as np
data = pd.read_csv('Training_examples.csv')
concepts = np.array(data)[:,:-1]
#print(concepts)
target = np.array(data)[:,-1]
def train(con,tar):
 for i,val in enumerate(tar):
 if val=='Yes':
 specific_h = con[i].copy()
 break

 for i,val in enumerate(con):
 if tar[i]=='Yes':
 for x in range(len(specific_h)):
 if val[x] != specific_h[x]:
 specific_h[x] = '?'
 else:
 pass
 return specific_h
print(train(concepts, target)) 

program 5
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
import warnings
warnings.filterwarnings('ignore')
# Step 1: Generate dataset
np.random.seed(42)
values = np.random.rand(100)
labels = []
for i in range(50): # Iterate over the first 50 values
 if values[i] <= 0.5:
 labels.append('Class1')
 else:
 labels.append('Class2')
# Append None for the remaining 50 values
labels += [None] * 50
# Create DataFrame
data = {
 "Point": [f"x{i+1}" for i in range(100)],
 "Value": values,
 "Label": labels
}
df = pd.DataFrame(data)
# Split data into labeled and unlabeled
labeled_df = df[df["Label"].notna()].copy()
X_train = labeled_df[["Value"]].values
y_train = labeled_df["Label"].values
unlabeled_df = df[df["Label"].isna()].copy()
X_test = unlabeled_df[["Value"]].values #or to_numpy()
# Generate true labels for accuracy calculation
true_labels = ["Class1" if x <= 0.5 else "Class2" for x in values[50:]]
# Step 2: Perform KNN classification for different values of k
k_values = [1, 2, 3, 4, 5, 20, 30]
results = {}
accuracies = {}
for k in k_values:
 knn = KNeighborsClassifier(n_neighbors=k)
 knn.fit(X_train, y_train)
 predictions = knn.predict(X_test)

 # Store results
 results[k] = predictions

 # Calculate accuracy
 accuracy = accuracy_score(true_labels, predictions) * 100
 accuracies[k] = accuracy
 print(f"Accuracy for k={k}: {accuracy:.2f}%")

 # Assign predictions back to the original DataFrame
 df.loc[df["Label"].isna(), f"Label_k{k}"] = predictions

# Display the final predictions
print(df.tail(10)) 

program 6
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
def kernel(point,xmat, k):
 m,n = np.shape(xmat)
 weights = np.asmatrix(np.eye((m))) # eye - identity matrix
 for j in range(m):
 diff = point - X[j]
 weights[j,j] = np.exp(diff*diff.T/(-2.0*k**2))
 return weights
def localWeight(point,xmat,ymat,k):
 wei = kernel(point,xmat,k)
 W = (X.T*(wei*X)).I*(X.T*(wei*ymat.T))
 return W
def localWeightRegression(xmat,ymat,k):
 m,n = np.shape(xmat)
 ypred = np.zeros(m)
 for i in range(m):
 ypred[i] = xmat[i]*localWeight(xmat[i],xmat,ymat,k)
 return ypred
def graphPlot(X,ypred):
 sortindex = X[:,1].argsort(0) #argsort - index of the smallest
 xsort = X[sortindex][:,0]
 fig = plt.figure()
 ax = fig.add_subplot(1,1,1)
 ax.scatter(bill,tip, color='green')
 ax.plot(xsort[:,1],ypred[sortindex], color = 'red', linewidth=5)
 plt.xlabel('Total bill')
 plt.ylabel('Tip')
 plt.show()
# load data points
data = pd.read_csv(â€˜tips.csv')
bill = np.array(data.total_bill) # We use only Bill amount and Tips data tip =
np.array(data.tip)
tip = np.array(data.tip)
mbill = np.asmatrix(bill) # .mat will convert nd array is converted in 2D array mtip =
np.asmatrix(tip)
mtip = np.asmatrix(tip)
m= np.shape(mbill)[1]
one = np.asmatrix(np.ones(m))
X = np.hstack((one.T,mbill.T)) # 244 rows, 2 cols
ypred = localWeightRegression(X,mtip,1.5) # increase k to get smooth curves
graphPlot(X,ypred) 

program 9
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import fetch_olivetti_faces
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
# Step 1: Load Olivetti Faces
data = fetch_olivetti_faces()
images = data.images
targets = data.target
# Step 2: Show 10 sample images
fig, axes = plt.subplots(2, 5, figsize=(10, 5))
fig.suptitle("Sample Images from Olivetti Faces Dataset", fontsize=16)
for i, ax in enumerate(axes.flat):
 ax.imshow(images[i], cmap='gray')
 ax.set_title(f"Person ID: {targets[i]}")
 ax.axis('off')
plt.tight_layout()
plt.subplots_adjust(top=0.85)
plt.show()
# Step 3: Flatten the image data (64x64 -> 4096)
X = images.reshape((images.shape[0], -1)) # shape: (400, 4096)
y = targets
# Step 4: Train-test split
X_train, X_test, y_train, y_test = train_test_split(
 X, y, test_size=0.2, stratify=y, random_state=42
)
# Step 5: Train Gaussian Naive Bayes model
model = GaussianNB()
model.fit(X_train, y_train)
# Step 6: Predict on test data
y_pred = model.predict(X_test)
# Step 7: Evaluate performance
accuracy = accuracy_score(y_test, y_pred)
print(f"\nAccuracy of Naive Bayes: {accuracy:.2f}")
print("\nClassification Report:\n", classification_report(y_test, y_pred))
# Create a DataFrame to compare actual vs predicted
comparison = pd.DataFrame({
 "Index": np.arange(len(y_test)),
 "Actual (y_test)": y_test,
 "Predicted (y_pred)": y_pred
})
print(comparison.head(20)) # Show first 20 rows
comparison["Match"] = comparison["Actual (y_test)"] == comparison["Predicted (y_pred)"]
print(comparison.head(20))

Program 10
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
import warnings
warnings.filterwarnings('ignore')
data = pd.read_csv("Breast_Cancer_Dataset.csv")
data.head()
data.info()
data.diagnosis.unique()
data.isnull().sum()
data.duplicated().sum()
df = data.drop(['id'], axis=1)
df['diagnosis'] = df['diagnosis'].map({'M':1, 'B':0}) # Malignant:1, Benign:0
#dropped the Diagnosis (target) since clustering is unsupervised.
df.drop(columns=["diagnosis"], inplace=True) # Removing Target
# Standardize the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df)
# Apply PCA for Dimensionality Reduction
pca = PCA(n_components=2) # Reduce to 2 dimensions for visualization
X_pca = pca.fit_transform(X_scaled)
# Check explained variance ratio
explained_variance = pca.explained_variance_ratio_
total_explained_variance = np.sum(explained_variance)
print(f"Variance explained by PC1: {explained_variance[0]:.4f}")
print(f"Variance explained by PC2: {explained_variance[1]:.4f}")
#Apply K-Means Clustering with the optimal k (usually where elbow occurs, k=2)
kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)
clusters = kmeans.fit_predict(X_pca)
# Step 7: Visualize the Clusters
plt.figure(figsize=(8, 6))
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=clusters, cmap="viridis", alpha=0.6)
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=200)
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.title("K-Means Clustering after PCA")
plt.show() 